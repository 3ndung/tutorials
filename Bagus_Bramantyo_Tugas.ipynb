{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bagus_Bramantyo_Tugas",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3ndung/tutorials/blob/master/Bagus_Bramantyo_Tugas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXA_NBB3GSY5"
      },
      "source": [
        "# Nama : Bagus Bramantyo  \n",
        "# NPM : 19312039  \n",
        "# Kelas : IF Gab 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82pNhuV4PfZH"
      },
      "source": [
        "# **IMPORT LIBRARY YANG PENTING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QItU7uqr8mHO",
        "outputId": "627f4c30-2c0d-4b5a-acb7-8ca895d5dbb4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz55jCwDPvCw"
      },
      "source": [
        "**# Contoh Sentence/ Kalimat yang akan di Training atau process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QovFX8VV8pRa"
      },
      "source": [
        "\n",
        "sentences = [\"We are reading about Natural Language Processing Here\",\n",
        "            \"Natural Language Processing making computers comprehend language data\",\n",
        "            \"The field of Natural Language Processing is evolving everyday\"]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW9bYS85P3d7"
      },
      "source": [
        "**# Memasukan Sentence ke dalam object list menggunakan library pandas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v1pfRm18vT2",
        "outputId": "5c6c0e3e-4cd1-4799-fe38-cb8372d31c4f"
      },
      "source": [
        "corpus = pd.Series(sentences)\n",
        "corpus"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    We are reading about Natural Language Processi...\n",
              "1    Natural Language Processing making computers c...\n",
              "2    The field of Natural Language Processing is ev...\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15ctakV-QKQQ"
      },
      "source": [
        "**# Membuat Fungsi untuk alfabet, number & beberapa kata khusus**  \n",
        "input adalah corpus diatas  \n",
        "outputnya adalah text corpus yang sudah di bersihkan "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34QWiRcN82zj"
      },
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "\n",
        "    cleaned_corpus = pd.Series()                                                 # ini Series kosong untuk menampung corups output bersih yang dihasilkan\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:                                           #Keep list adalah list bawaan dari library nltk\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)        # menggunakan regex untuk mengambil pola alfabet dan numbers saja\n",
        "                p1 = p1.lower()                                                 # menggubah ke dalam no captital\n",
        "                qs.append(p1)                                                   # memasukan p1 ke array qs\n",
        "            else : qs.append(word)\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUFgCsUcRnBQ"
      },
      "source": [
        "**# membuat lemmatizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZIB7nuX84r-"
      },
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()                                                   # WordNetLemmatizer bawaan NLTK\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]         # me-lemmitizer corpus\n",
        "    return corpus"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l01TaNkSR-Jb"
      },
      "source": [
        "**# membuat Steam function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFoMIOa68-sb"
      },
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTv5MCA5SJlq"
      },
      "source": [
        "**# membuat/ meggunakan stop_words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yvVzY6q9JXK"
      },
      "source": [
        "def stopwords_removal(corpus):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4FPwfnqSUnY"
      },
      "source": [
        "**# membuat fungsi preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LmWIdJ79Qvu"
      },
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "    \n",
        "    Input : \n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "    \n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "    \n",
        "    Output : Returns the processed text corpus\n",
        "    \n",
        "    '''\n",
        "    print(f'ini keep list {keep_list}')\n",
        "    \n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "    \n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "    \n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "        \n",
        "        \n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "    \n",
        "    corpus = [' '.join(x) for x in corpus]        \n",
        "\n",
        "    return corpus"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ZIHIc-9VZX",
        "outputId": "5007adfc-4d89-405c-cbe7-30c3eee91477"
      },
      "source": [
        "preprocessed_corpus = preprocess(corpus, keep_list = [], stemming = False, stem_type = None,\n",
        "                                lemmatization = True, remove_stopwords = True)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ini keep list []\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Ini Adalah Output preprocessed_corpus -->  ['read natural language process', 'natural language process make computers comprehend language data', 'field natural language process evolve everyday'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJKMpsqTUUla"
      },
      "source": [
        "**# Contoh Output Preprocessed_Corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOWUbsrqURPX",
        "outputId": "e5d04ae9-9b95-4684-9223-4980bcc8695b"
      },
      "source": [
        "print(f'Ini Adalah Output preprocessed_corpus -->  {preprocessed_corpus} \\n')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ini Adalah Output preprocessed_corpus -->  ['read natural language process', 'natural language process make computers comprehend language data', 'field natural language process evolve everyday'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv93-CTrUk8n"
      },
      "source": [
        "**# Vectorize**  \n",
        "Menghitung kata kata yang divektorisasi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v7j0xhNU_Km"
      },
      "source": [
        "**# Mendfinisi Vectorize & bow_matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJqsnAOt9ZUr"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Q_wOQm9gno",
        "outputId": "49d34133-9f8a-4c0a-fb94-f456a9cfc557"
      },
      "source": [
        "print('Kata Kata yang sering muncul dalam sentence / Corpus \\n')\n",
        "print(vectorizer.get_feature_names())\n",
        "print('\\n Konvert dalam bentuk Matrix \\n')\n",
        "print(bow_matrix.toarray())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kata Kata yang sering muncul dalam sentence / Corpus \n",
            "\n",
            "['comprehend', 'computers', 'data', 'everyday', 'evolve', 'field', 'language', 'make', 'natural', 'process', 'read']\n",
            "\n",
            " Konvert dalam bentuk Matrix \n",
            "\n",
            "[[0 0 0 0 0 0 1 0 1 1 1]\n",
            " [1 1 1 0 0 0 2 1 1 1 0]\n",
            " [0 0 0 1 1 1 1 0 1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zjA3wX49ksJ",
        "outputId": "aba2de81-feed-44fe-d8fe-4a5379252e22"
      },
      "source": [
        "print(bow_matrix.toarray().shape)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpkaEmHX9szt"
      },
      "source": [
        "vectorizer_ngram_range = CountVectorizer(analyzer='word', ngram_range=(1,3))\n",
        "bow_matrix_ngram = vectorizer_ngram_range.fit_transform(preprocessed_corpus)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdnxo4Cv9wgO",
        "outputId": "584f8fcc-aabe-443d-bccc-e3c22a2315a1"
      },
      "source": [
        "\n",
        "print(vectorizer_ngram_range.get_feature_names())\n",
        "print(bow_matrix_ngram.toarray())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['comprehend', 'comprehend language', 'comprehend language data', 'computers', 'computers comprehend', 'computers comprehend language', 'data', 'everyday', 'evolve', 'evolve everyday', 'field', 'field natural', 'field natural language', 'language', 'language data', 'language process', 'language process evolve', 'language process make', 'make', 'make computers', 'make computers comprehend', 'natural', 'natural language', 'natural language process', 'process', 'process evolve', 'process evolve everyday', 'process make', 'process make computers', 'read', 'read natural', 'read natural language']\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1]\n",
            " [1 1 1 1 1 1 1 0 0 0 0 0 0 2 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG5unpk79ySr"
      },
      "source": [
        "\n",
        "vectorizer_max_features = CountVectorizer(analyzer='word', ngram_range=(1,3), max_features = 6)\n",
        "bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoBBOMEA92D8",
        "outputId": "8225dc30-754d-46b0-bd24-6b25b7db737c"
      },
      "source": [
        "print(vectorizer_max_features.get_feature_names())\n",
        "print(bow_matrix_max_features.toarray())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['language', 'language process', 'natural', 'natural language', 'natural language process', 'process']\n",
            "[[1 1 1 1 1 1]\n",
            " [2 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZkPMkqi97L5"
      },
      "source": [
        "vectorizer_max_features = CountVectorizer(analyzer='word', ngram_range=(1,3), max_df = 3, min_df = 2)\n",
        "bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0EnOak39_ja",
        "outputId": "f4f4d2f9-f500-46ef-f2bd-3ed3b4014bb8"
      },
      "source": [
        "print(vectorizer_max_features.get_feature_names())\n",
        "print(bow_matrix_max_features.toarray())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['language', 'language process', 'natural', 'natural language', 'natural language process', 'process']\n",
            "[[1 1 1 1 1 1]\n",
            " [2 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}